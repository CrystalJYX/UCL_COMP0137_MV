{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import io\n",
    "import cv2 as cv \n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import os # for reading all files in a folder\n",
    "#pylab.rcParams['figure.figsize'] = (12.0, 10.0)\n",
    "\n",
    "# plots are too big to save, this file cannot be reopened, \n",
    "# so I changed the size of plots\n",
    "pylab.rcParams['figure.figsize'] = (6.0, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condensation 9c\n",
    "\n",
    "You'll have to call the code below four times (once each with 'll','lr', 'ul', 'ur': lower left, lower right, upper left, ipper right). For each time you'll get an estimated track (2D locations over time) of one of the corners of the pattern moved. Note that we're saving off the single MAP estimate at each frame, but the particles in the Particle Filter are attempting to keep track of the whole posterior distribution. \n",
    "\n",
    "Most TO DO parts below should be copy-paste from Practical9a and Practical9b.\n",
    "\n",
    "computeLikelihood has been modified from its previous versions to cope with these particular gray-scale images.\n",
    "\n",
    "\n",
    "Qustions you could ask yourself (not a TO DO): \n",
    "- Can you find a pattern between the movement of the particles and the movement of the camera relative to the scene?\n",
    "- When is the MAP estimate especially good/bad?\n",
    "- What is the relationship to the amount of noise? (--> experiment with different levels!)\n",
    "- Why do different numbers of particles lead to very differenet results (for example 50 vs. 2000)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Likelihood function is simple patch similarity\n",
    "\n",
    "def computeLikelihood(image, template):\n",
    "    #opencv's available methods - experiment with these\n",
    "    #careful what range the output is!\n",
    "    methods = [cv.TM_CCOEFF, cv.TM_CCOEFF_NORMED, cv.TM_CCORR,\n",
    "            cv.TM_CCORR_NORMED, cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]\n",
    "    \n",
    "    likelihood = cv.matchTemplate(image[:,:,2], template, methods[0])\n",
    "    # (You can also try converting the image to greyscale instead of using the third channel as above with \n",
    "    # cv.cvtColor(image, cv.COLOR_BGR2GRAY))\n",
    "    \n",
    "    #we can pad to make this the size of the input image (for easier indexing)   \n",
    "    pad_first = int(template.shape[0])\n",
    "    pad_second = int(template.shape[1])\n",
    "    pad_amounts = ((0, pad_first-1), (0, pad_second-1))\n",
    "    likelihood = np.pad(likelihood, pad_amounts, 'constant')\n",
    "    likelihood[likelihood<0] = 0 # to avoid negative weights \n",
    "    \n",
    "    # apply a 10x10 averaging filter for stability. You can experiment with different sizes. \n",
    "    kernel = np.ones((10,10),np.float32)/100\n",
    "    smoothed = cv.filter2D(likelihood,-1,kernel) \n",
    "    return smoothed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def HW2_Practical9c(corner):\n",
    "    template = sp.io.loadmat(corner+'.mat')['pixelsTemplate']\n",
    "    #let's show the template\n",
    "    print('We are matching this template with shape: ', template.shape)\n",
    "    plt.imshow(template)\n",
    "    plt.show()\n",
    "\n",
    "    # Load all images in folder\n",
    "    images = []\n",
    "    iFrame = 0\n",
    "    folder = 'Pattern01/'\n",
    "    lst = os.listdir(folder)\n",
    "    lst.sort()\n",
    "\n",
    "    for frameNum in lst:\n",
    "        images.append(cv.imread(folder+frameNum))\n",
    "        iFrame += 1\n",
    "    # plot first image \n",
    "    plt.imshow(images[0])\n",
    "    plt.show()\n",
    "\n",
    "    imgHeight, imgWidth, colors = images[0].shape\n",
    "    numParticles = 2000;\n",
    "    weight_of_samples = np.ones((numParticles,1))\n",
    "\n",
    "    # TO DO: normalize the weights (may be trivial this time) [done]\n",
    "    #weight_of_samples = weight_of_samples #replace this \n",
    "    weight_of_samples = weight_of_samples/np.sum(weight_of_samples)\n",
    "\n",
    "\n",
    "    # Initialize which samples from \"last time\" we want to propagate: all of\n",
    "    # them!:\n",
    "    samples_to_propagate = range(0, numParticles)\n",
    "\n",
    "\n",
    "    # ============================\n",
    "    # NOT A TO DO: You don't need to change the code below, but eventually you may\n",
    "    # want to vary the number of Dims (compare for example to lab 9b) \n",
    "    numDims_w = 2;\n",
    "    # Here we randomly initialize some particles throughout the space of w:\n",
    "    particles_old = np.random.rand(numParticles, numDims_w)\n",
    "    particles_old[:,0] = particles_old[:,0] * imgHeight\n",
    "    particles_old[:,1] = particles_old[:,1] * imgWidth\n",
    "    # ============================\n",
    "\n",
    "    #Initialize a temporary array r to store the per-frame MAP estimate of w. This is what we'll return in the end.\n",
    "    r = np.zeros((iFrame, numDims_w));\n",
    "\n",
    "    for iTime in range(iFrame):\n",
    "        print('Processing Frame', iTime)\n",
    "        # TO DO: compute the cumulative sume of the weights. [done]\n",
    "        #cum_hist_of_weights = np.linspace(0, 1, numParticles) # replace this\n",
    "        #print(weight_of_samples)\n",
    "        cum_hist_of_weights = np.cumsum(weight_of_samples)\n",
    "        \n",
    "\n",
    "        # ==============================================================\n",
    "        # Resample the old distribution at time t-1, and select samples, favoring\n",
    "        # those that had a higher posterior probability.\n",
    "        # ==============================================================\n",
    "        samples_to_propagate = np.zeros(numParticles, dtype=np.int32)\n",
    "\n",
    "        # Pick random thresholds in the cumulative probability's range [0,1]:\n",
    "        some_threshes = np.random.rand(numParticles)\n",
    "\n",
    "\n",
    "        # For each random threshold, find which sample in the ordered set is\n",
    "        # the first one to push the cumulative probability above that\n",
    "        # threshold, e.g. if the cumulative histogram goes from 0.23 to 0.26\n",
    "        # between the 17th and 18th samples in the old distribution, and the\n",
    "        # threshold is 0.234, then we'll want to propagate the 18th sample's w\n",
    "        # (i.e. particle #18).\n",
    "\n",
    "        for sampNum in range(numParticles): \n",
    "            thresh = some_threshes[sampNum]\n",
    "            for index in range (numParticles):\n",
    "                if cum_hist_of_weights[index] > thresh:\n",
    "                    break\n",
    "            samples_to_propagate[sampNum] = index\n",
    "\n",
    "        # Note: it's ok if some of the old particles get picked repeatedly, while\n",
    "        # others don't get picked at all.\n",
    "\n",
    "\n",
    "        # =================================================\n",
    "        # Visualize if you want\n",
    "        # =================================================\n",
    "        #plt.title('Cumulative histogram of probabilities for sorted list of particles')\n",
    "        #plt.plot(np.zeros(numParticles), some_threshes,'b.')\n",
    "        #plt.plot(range(0, numParticles), cum_hist_of_weights, 'rx-')\n",
    "        #which_sample_ids = np.unique(samples_to_propagate)\n",
    "        #how_many_of_each = np.bincount(np.ravel(samples_to_propagate))\n",
    "        #for k in range(len(which_sample_ids)):\n",
    "        #    plt.plot(which_sample_ids[k], 0, 'bo-', markersize = 3 * how_many_of_each[k], markerfacecolor='white')\n",
    "        #plt.xlabel('Indeces of all available samples, with larger blue circles for frequently re-sampled particles\\n(Iteration %01d)' % iTime)\n",
    "        #plt.ylabel('Cumulative probability');\n",
    "        #plt.show()\n",
    "        # =================================================\n",
    "        # =================================================\n",
    "\n",
    "        # Predict where the particles we sampled from the old distribution of \n",
    "        # state-space will go in the next time-step. This means we have to apply \n",
    "        # the motion model to each old sample.\n",
    "        particles_new = np.zeros_like(particles_old)\n",
    "        for particleNum in range(numParticles):\n",
    "            # TO DO: Incorporate some noise, e.g. Gaussian noise with std 20,\n",
    "            # into the current location (particles_old), to give a Brownian\n",
    "            # motion model.\n",
    "            #particles_new[particleNum, :] =  particles_old[particleNum, :] # replace this \n",
    "            noise = np.random.normal(0,20,2)\n",
    "            particles_new[particleNum, :] =  particles_old[samples_to_propagate[particleNum], :] + noise\n",
    "\n",
    "            \n",
    "        # TO DO: Not initially, but change the motion model above to have\n",
    "        # different degrees of freedom, and optionally completely different\n",
    "        # motion models. See Extra Credit for more instructions.\n",
    "\n",
    "        #calculate likelihood function\n",
    "        likelihood = computeLikelihood(images[iTime], template)\n",
    "\n",
    "        #plot results\n",
    "        f, axarr = plt.subplots(1, 2)\n",
    "        axarr[0].imshow(images[iTime])\n",
    "        axarr[0].set_title('Particles')\n",
    "        # now draw the particles onto the image\n",
    "        axarr[0].plot(particles_new[:,1]+template.shape[1]/2, particles_new[:,0]+template.shape[0]/2, 'rx')\n",
    "\n",
    "        #plot the likelihood\n",
    "        axarr[1].imshow(likelihood)\n",
    "        axarr[1].set_title('Likelihood')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # From here we incorporate the data for the new state (time t):\n",
    "        # The new particles accompanying predicted locations in state-space\n",
    "        # for time t, are missing their weights: how well does each particle\n",
    "        # explain the observations x_t?\n",
    "        for particleNum in range(numParticles):\n",
    "\n",
    "            # Convert the particle from state-space w to measurement-space x:\n",
    "            # Note: that step is trivial here since both are in 2D space of image\n",
    "            # coordinates\n",
    "\n",
    "            # Within the loop, we evaluate the likelihood of each particle:\n",
    "            particle = particles_new[particleNum, :]\n",
    "            # Check that the predicted location is a place we can really evaluate\n",
    "            # the likelihood.\n",
    "            inFrame = particle[0] >= 0.0 and  particle[0] <= imgHeight and particle[1] >= 0.0 and particle[1] <= imgWidth\n",
    "            if inFrame:\n",
    "                minX = particle[1]\n",
    "                minY = particle[0]\n",
    "\n",
    "                weight_of_samples[particleNum] = likelihood[int(minY), int(minX)]\n",
    "\n",
    "            else:\n",
    "                weight_of_samples[particleNum] = 0.0\n",
    "\n",
    "        # TO DO: normalize the weights [done]\n",
    "        #weight_of_samples = weight_of_samples # replace this\n",
    "        weight_of_samples = weight_of_samples/np.sum(weight_of_samples)\n",
    "\n",
    "        # find the location of the particle with highest weight\n",
    "        indices = np.argsort(weight_of_samples,0)\n",
    "        bestScoringParticles = particles_new[np.squeeze(indices[-15:]), :]\n",
    "        plt.plot(bestScoringParticles[-1:,1], bestScoringParticles[-1:,0], 'rx')\n",
    "        # Return the MAP of middle position. Add template.shape/2 because matchTemplate finds the position of the upper left corner \n",
    "        # of the template. We want to plot the centre of the template. \n",
    "        r[iTime,:] = bestScoringParticles[-1,1]+template.shape[1]/2,bestScoringParticles[-1,0]+template.shape[0]/2\n",
    "        print(r[iTime,:])   \n",
    "        plt.show()\n",
    "\n",
    "        #print the original image and the position of the tracked corner.\n",
    "        plt.imshow(images[iTime])\n",
    "        plt.plot(r[iTime,0],r[iTime,1],'rx')\n",
    "        plt.show()\n",
    "        # Now we're done updating the state for time t. \n",
    "        # For Condensation, just clean up and prepare for the next round of \n",
    "        # predictions and measurements:\n",
    "        particles_old = particles_new\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = HW2_Practical9c('ul')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: \n",
    "The plots above show an estimated track (2D locations over time) of one of the corners of the pattern moved. The particle randonly distribute in the first picture and then they move to the area which has high posterior. And finally, all the points concentrate on the upper left corner.\n",
    "The plots below show the same method working on lower left corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = HW2_Practical9c('ll')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
